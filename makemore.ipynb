{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**MAKEMORE: An autoregressive character-level language model**\n",
        "\n",
        "We will use **Bigrams and MLP** in this notebook.\n",
        "\n",
        "Name: **Lakshit Sethia**\n",
        "\n",
        "Roll No.: **210102123**\n",
        "\n",
        "DA623: Final Project"
      ],
      "metadata": {
        "id": "FVaTImwEj5Ye"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEtqsDmDA7_M"
      },
      "source": [
        "In this we are basically going to do next-prediction based on characters. So for each letter, we will be providing a large dataset of names of people, and it will be able to generate many more related names.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbQgLOJ6A7_M"
      },
      "source": [
        "**Reading and exploring the dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have created a dataset containing Indian Names by combining and refining multiple datasets. The final data is in the file ***names.txt***.\n",
        "\n",
        "[1] www.kaggle.com/datasets/ananysharma/indian-names-dataset\n",
        "\n",
        "[2] www.kaggle.com/datasets/jasleensondhi/indian-names-corpus-nltk-data\n",
        "\n",
        "[3] www.kaggle.com/datasets/shubhamuttam/indian-names-by-gender\n"
      ],
      "metadata": {
        "id": "kFnxnkoBjDEM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zq7dh192A7_N"
      },
      "outputs": [],
      "source": [
        "words = open('names.txt', 'r').read().splitlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wKZ6JbCA7_N"
      },
      "outputs": [],
      "source": [
        "words[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikcJmxe_A7_O"
      },
      "outputs": [],
      "source": [
        "len(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oyCr9p6A7_P"
      },
      "outputs": [],
      "source": [
        "min(len(word) for word in words)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2EMILG3Cdpf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQsTB6HjA7_P"
      },
      "outputs": [],
      "source": [
        "max(len(word) for word in words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CReWzlHKA7_Q"
      },
      "source": [
        "**Exploring the bigrams in the dataset**\n",
        "\n",
        "Two columns are formed where The first character is taken and is zipped with its next one. So when there is only one character left, then it exits.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqTIfMnVA7_Q"
      },
      "outputs": [],
      "source": [
        "for word in words[:1]:  #Example\n",
        "    for ch1, ch2 in zip(word, word[1:]):\n",
        "        print(ch1, ch2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0H9OSJTfduvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are just adding our own set of characters in the list, so as to get like this custom output - Labelling the first and the last character of the word"
      ],
      "metadata": {
        "id": "Wuw3tsm5dzdu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OY2pedfMA7_R"
      },
      "outputs": [],
      "source": [
        "for word in words[:1]:\n",
        "    chs = ['<S>'] + list(word) + ['<E>']\n",
        "    for ch1, ch2 in zip(chs, chs[1:]):\n",
        "        print(ch1, ch2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5q8IxEuaA7_R"
      },
      "outputs": [],
      "source": [
        "for word in words[:3]:\n",
        "    chs = ['<S>'] + list(word) + ['<E>']\n",
        "    for ch1, ch2 in zip(chs, chs[1:]):\n",
        "        print(ch1, ch2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Paec1GT4A7_R"
      },
      "source": [
        "**Counting bigrams in a python dictionary**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have created a dictionary 'b' - First data structure.\n",
        "\n",
        "Then we have created a set 'bigram' which is just a set of two characters - Second data structure.\n",
        "\n",
        "Here we are adding it to the dictionary 'b', where the key is 'bigram' (which is a set of character pairs) has the value counts of the set occoured (The number of times that set has occured)."
      ],
      "metadata": {
        "id": "UkrZYwdpd3S9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-eHTAIcA7_S"
      },
      "outputs": [],
      "source": [
        "b = {}  #dictionary 'b'\n",
        "for word in words[:1]:\n",
        "    chs = ['<S>'] + list(word) + ['<E>']\n",
        "    for ch1, ch2 in zip(chs, chs[1:]):\n",
        "        bigram = (ch1, ch2) #bigram\n",
        "        b[bigram] = b.get(bigram, 0) + 1\n",
        "        print(ch1, ch2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOLz5iRcA7_S"
      },
      "source": [
        "**Note:**   \\\n",
        "`b.get(bigram)` is the same as `b[bigram]`  \\\n",
        "Just that here: `b.get(bigram, 0)` if we don't get a bigram value, we want it to assign to 0    \\\n",
        "Finally we are adding one `+1` as we want to count the occurance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKTtJoHZA7_S"
      },
      "outputs": [],
      "source": [
        "b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1ZZbnMlA7_S"
      },
      "outputs": [],
      "source": [
        "b = {}\n",
        "for word in words[:3]:\n",
        "    chs = ['<S>'] + list(word) + ['<E>']\n",
        "    for ch1, ch2 in zip(chs, chs[1:]):\n",
        "        bigram = (ch1, ch2)\n",
        "        b[bigram] = b.get(bigram, 0) + 1\n",
        "        print(ch1, ch2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANDwiFx6A7_T"
      },
      "outputs": [],
      "source": [
        "b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STU3qiqBA7_T"
      },
      "outputs": [],
      "source": [
        "b = {}\n",
        "for word in words:\n",
        "    chs = ['<S>'] + list(word) + ['<E>']\n",
        "    for ch1, ch2 in zip(chs, chs[1:]):\n",
        "        bigram = (ch1, ch2)\n",
        "        b[bigram] = b.get(bigram, 0) + 1\n",
        "\n",
        "# b"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We know 'b' is a dictionary. So .items() basically gives us that it's values in a (Key, Value) set"
      ],
      "metadata": {
        "id": "6PmDJqOjeU43"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubmUWfBmA7_T"
      },
      "outputs": [],
      "source": [
        "b.items()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now this by default sorts the values based on the Key"
      ],
      "metadata": {
        "id": "pB7kUVJmec_p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bb6KCOspA7_T"
      },
      "outputs": [],
      "source": [
        "sorted(b.items())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Now, here we are specifying we want to sort based on the values. So we select the key, then in the lambda function, we take the keyvalue (kv) and select the second element in the set, which is what we want\n",
        " - sign is for descending"
      ],
      "metadata": {
        "id": "SQ6-gTL3ehsR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hbe5xBbA7_U"
      },
      "outputs": [],
      "source": [
        "sorted(b.items(), key= lambda kv: -kv[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVyKUZi_A7_U"
      },
      "source": [
        "**Counting bigrams in a 2D torch tensor (\"training the model\")**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FN18HP8eA7_U"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AtXfUsoKA7_U"
      },
      "outputs": [],
      "source": [
        "N = torch.zeros((28, 28), dtype = torch.int32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giUOnIK4A7_U"
      },
      "outputs": [],
      "source": [
        "chars = sorted(list(set(''.join(words))))\n",
        "\n",
        "stoi = {s:i for i,s in enumerate(chars)}\n",
        "stoi['<S>'] = 26\n",
        "stoi['<E>'] = 27"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRu_PJQdA7_U"
      },
      "outputs": [],
      "source": [
        "stoi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0d6XQ_CA7_V"
      },
      "outputs": [],
      "source": [
        "for word in words:\n",
        "    chs = ['<S>'] + list(word) + ['<E>']\n",
        "    for ch1, ch2 in zip(chs, chs[1:]):\n",
        "        ix1 = stoi[ch1]\n",
        "        ix2 = stoi[ch2]\n",
        "        N[ix1, ix2] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r50FeRnkA7_V"
      },
      "outputs": [],
      "source": [
        "N"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uqF1c_xA7_V"
      },
      "source": [
        "**Visualizing the bigram tensor**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hE-eW1N0A7_V"
      },
      "outputs": [],
      "source": [
        "itos = {i:s for s,i in stoi.items()}\n",
        "itos"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bigram Plot"
      ],
      "metadata": {
        "id": "RFEVroeNe086"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DyFjDvtAA7_V"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "plt.figure(figsize=(16,16))\n",
        "plt.imshow(N, cmap='Blues')\n",
        "for i in range(28):\n",
        "    for j in range(28):\n",
        "        chstr = itos[i] + itos[j]\n",
        "        plt.text(j, i, chstr, ha=\"center\", va=\"bottom\", color='gray')\n",
        "        plt.text(j, i, N[i, j].item(), ha=\"center\", va=\"top\", color='gray')\n",
        "plt.axis('off');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLJgcAk8A7_W"
      },
      "source": [
        "**Deleting spurious (S) and (E) tokens in favor of a single . token**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcPOxxa2A7_W"
      },
      "outputs": [],
      "source": [
        "N = torch.zeros((27, 27), dtype = torch.int32)\n",
        "\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
        "stoi['.'] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlIGUx6CA7_W"
      },
      "outputs": [],
      "source": [
        "itos = {i:s for s,i in stoi.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhkGiJ-kA7_W"
      },
      "outputs": [],
      "source": [
        "for word in words:\n",
        "    chs = ['.'] + list(word) + ['.']\n",
        "    for ch1, ch2 in zip(chs, chs[1:]):\n",
        "        ix1 = stoi[ch1]\n",
        "        ix2 = stoi[ch2]\n",
        "        N[ix1, ix2] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDQkuWkBA7_W"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "plt.figure(figsize=(16,16))\n",
        "plt.imshow(N, cmap='Blues')\n",
        "for i in range(27):\n",
        "    for j in range(27):\n",
        "        chstr = itos[i] + itos[j]\n",
        "        plt.text(j, i, chstr, ha=\"center\", va=\"bottom\", color='gray')\n",
        "        plt.text(j, i, N[i, j].item(), ha=\"center\", va=\"top\", color='gray')\n",
        "plt.axis('off');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DP2yDUGUA7_W"
      },
      "source": [
        "**Sampling from the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wa0gRZCMA7_W"
      },
      "outputs": [],
      "source": [
        "N[0]    #Viewing just the first row"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzRLO_yJA7_X"
      },
      "source": [
        "First we make them all into float. \\\n",
        "Then we make a probability distribution \\\n",
        "We do that by dividing `p` with `p.sum()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFUdVS7SA7_X"
      },
      "outputs": [],
      "source": [
        "p = N[0].float()\n",
        "p = p / p.sum()\n",
        "p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYw5VU6gA7_X"
      },
      "outputs": [],
      "source": [
        "p.sum().item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrCpKL4BA7_X"
      },
      "source": [
        "So the total probability sums up to 1. Therefore now we have the probability values for each of those characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2N0ywD-A7_X"
      },
      "outputs": [],
      "source": [
        "g = torch.Generator().manual_seed(210102123)\n",
        "p = torch.rand(3, generator=g)\n",
        "p = p / p.sum()\n",
        "p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDN5IRikA7_X"
      },
      "outputs": [],
      "source": [
        "torch.multinomial(p, num_samples=20, replacement=True, generator=g)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_IjaJ_3A7_Y"
      },
      "source": [
        "So based on the probability percentages `p` we get a bunch of sample values \\\n",
        "So `0` should be 60%, `1` should be 30%, `2` should be 10% of the total samples generated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkKCC6ViA7_Y"
      },
      "outputs": [],
      "source": [
        "g = torch.Generator().manual_seed(210102123)\n",
        "ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
        "itos[ix]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DUO-P6CA7_Y"
      },
      "source": [
        "So now, if we had got another sampled value, lets say 'm' (so we have taken the column), now we go to the row containing 'm' and then check for its correspondant character.    \\\n",
        "\\\n",
        "Keeping this jest in mind, we will be making this into a loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrBulpw6A7_Z"
      },
      "outputs": [],
      "source": [
        "g = torch.Generator().manual_seed(210102123)\n",
        "\n",
        "for i in range(10):\n",
        "    out = []\n",
        "    ix = 0\n",
        "    while True:\n",
        "        p = N[ix].float()\n",
        "        p = p / p.sum()\n",
        "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
        "        out.append(itos[ix])\n",
        "        if ix == 0:\n",
        "            break\n",
        "    print(''.join(out))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NygBco_jA7_Z"
      },
      "source": [
        "And this is why the Bigram model is so bad lol. The output generated not to great (Andrej said \"terrible\" xd), for example for `p.` , is that the model doesn't understand that 'p' should have had something before or after it. Right now, it considers that as a name itself."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkoc7bmxA7_Z"
      },
      "source": [
        "But now we will see why the output made by the model is not exactly too terrible"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9E3ADeZHA7_Z"
      },
      "outputs": [],
      "source": [
        "g = torch.Generator().manual_seed(210102123)\n",
        "\n",
        "for i in range(10):\n",
        "    out = []\n",
        "    ix = 0\n",
        "    while True:\n",
        "        # p = N[ix].float()\n",
        "        # p = p / p.sum()\n",
        "        p = torch.ones(27) / 27.0\n",
        "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
        "        out.append(itos[ix])\n",
        "        if ix == 0:\n",
        "            break\n",
        "    print(''.join(out))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4S9EIg7A7_Z"
      },
      "source": [
        "So now this is what we get when the model is completely untrained, it gives you a garbage of values.\n",
        "\n",
        "This is happening because we removed the probability distribution and added a distribution of uniform values. So all of the characters are equally likely to occur."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtKzTeJQA7_a"
      },
      "source": [
        "So yeah, if we train it with a Bigram, then its a lot better output. So ultimately it is actually working, just that Bigram is not so great for this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_B2gfKveA7_a"
      },
      "outputs": [],
      "source": [
        "#Solving the inefficiency problem\n",
        "P = N.float()\n",
        "P = P / P.sum(1, keepdim=True) #Here is where we are applying the sum and broadcasting rules. Sum for the function and Broadcasting for the division part that takes place.\n",
        "\n",
        "# 27 27\n",
        "# 27  1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KriboygEA7_a"
      },
      "outputs": [],
      "source": [
        "P[0].sum() #This should return the tensor object with value 1. So that entire row as been normalised"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-J0gbmBmA7_a"
      },
      "source": [
        "So the rule says:\n",
        "Two tensors are broadcastable if the following rules hold:\n",
        "\n",
        "Each tensor has at least one dimension.\n",
        "\n",
        "When iterating over the dimension sizes, starting at the trailing dimension, the dimension sizes must either be equal, one of them is 1, or one of them does not exist.\n",
        "\n",
        "So in our case, one of the dimentional sizes is one i.e 27 **1** \\\n",
        "And, the dimension sizes are equal: \\\n",
        "**27**  \\\n",
        "**27**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSBoxEJCA7_a"
      },
      "outputs": [],
      "source": [
        "g = torch.Generator().manual_seed(210102123)\n",
        "\n",
        "for i in range(10):\n",
        "    out = []\n",
        "    ix = 0\n",
        "    while True:\n",
        "        p = P[ix]\n",
        "        # p = N[ix].float()\n",
        "        # p = p / p.sum()\n",
        "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
        "        out.append(itos[ix])\n",
        "        if ix == 0:\n",
        "            break\n",
        "    print(''.join(out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zj-ExheXA7_b"
      },
      "outputs": [],
      "source": [
        "P = N.float()\n",
        "P /= P.sum(1, keepdim=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kH2Uj1UeA7_b"
      },
      "outputs": [],
      "source": [
        "g = torch.Generator().manual_seed(210102123)\n",
        "\n",
        "for i in range(10):\n",
        "    out = []\n",
        "    ix = 0\n",
        "    while True:\n",
        "        p = P[ix]\n",
        "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
        "        out.append(itos[ix])\n",
        "        if ix == 0:\n",
        "            break\n",
        "    print(''.join(out))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtDchFV6A7_b"
      },
      "source": [
        "--------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSVCQ3qYA7_b"
      },
      "source": [
        "**Evaluating our model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SeO42_crA7_b"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWpmLCksA7_b"
      },
      "outputs": [],
      "source": [
        "N = torch.zeros((27, 27), dtype=torch.int32)\n",
        "\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
        "stoi['.'] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgCu_H5XA7_b"
      },
      "outputs": [],
      "source": [
        "itos = {i:s for s,i in stoi.items()}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "  chs = ['.'] + list(word) + ['.']\n",
        "  for ch1, ch2 in zip(chs, chs[1:]):\n",
        "    ix1 = stoi[ch1]\n",
        "    ix2 = stoi[ch2]\n",
        "    N[ix1, ix2] += 1"
      ],
      "metadata": {
        "id": "n-vPWvlICDoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hTSUcxEA7_b"
      },
      "outputs": [],
      "source": [
        "P = N.float()\n",
        "P /= P.sum(1, keepdim=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Likelihood**: The probability of observing the data given the model.\n",
        "\n",
        "**Log-likelihood**: The natural logarithm of the likelihood.\n",
        "\n",
        "**Negative Log-likelihood (NLL)**: The negative of the log-likelihood"
      ],
      "metadata": {
        "id": "KiyLw_uJfWYk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ug_hwX3uA7_c"
      },
      "outputs": [],
      "source": [
        "log_likelihood = 0.0\n",
        "n = 0\n",
        "\n",
        "for word in words[:3]:\n",
        "    chs = ['.'] + list(word) + ['.']\n",
        "    for ch1, ch2 in zip(chs, chs[1:]):\n",
        "        ix1 = stoi[ch1]\n",
        "        ix2 = stoi[ch2]\n",
        "        prob = P[ix1, ix2] #Likelihood - product of all the values\n",
        "        logprob = torch.log(prob) #Log Likelihood\n",
        "        log_likelihood += logprob #Log Likelihood - Adding the logs of all probability values\n",
        "        n += 1 #Log Likelihood - For the average\n",
        "        print(f'{ch1}{ch2}: {prob:.4f} {logprob: .4f}')\n",
        "\n",
        "print(f'{log_likelihood=}')\n",
        "nll = -log_likelihood #Negative Log Likelihood\n",
        "print(f'{nll=}')\n",
        "print(f'{nll/n}') #Negative Log Likelihood - Average value\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model smoothening"
      ],
      "metadata": {
        "id": "oauLTlj0f1XC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GujnqGkPA7_c"
      },
      "outputs": [],
      "source": [
        "P = (N+1).float()\n",
        "P /= P.sum(1, keepdim=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcBTQdqPA7_c"
      },
      "outputs": [],
      "source": [
        "log_likelihood = 0.0\n",
        "n = 0\n",
        "\n",
        "# for word in words[:3]:\n",
        "for word in [\"lakshit\"]:\n",
        "    chs = ['.'] + list(word) + ['.']\n",
        "    for ch1, ch2 in zip(chs, chs[1:]):\n",
        "        ix1 = stoi[ch1]\n",
        "        ix2 = stoi[ch2]\n",
        "        prob = P[ix1, ix2] #Likelihood - product of all the values\n",
        "        logprob = torch.log(prob) #Log Likelihood\n",
        "        log_likelihood += logprob #Log Likelihood - Adding the logs of all probability values\n",
        "        n += 1 #Log Likelihood - For the average\n",
        "        print(f'{ch1}{ch2}: {prob:.4f} {logprob: .4f}')\n",
        "\n",
        "print(f'{log_likelihood=}')\n",
        "nll = -log_likelihood #Negative Log Likelihood\n",
        "print(f'{nll=}')\n",
        "print(f'{nll/n}') #Negative Log Likelihood - Average value\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we arrived at the model doing everything explicitly. We were performing counts and we were normalizing those counts.\n",
        "\n",
        "Now, we'll be doing an alternative approach but the final output will be the same.\n",
        "\n",
        "**Here we are going to cast the problem of Bigram Character level language modelling into a neural network**\n",
        "\n",
        "So our NN will still be a character level language model.\n",
        "\n",
        "So we have an input character -> given to the neural network and then it is gonna predict the probability -> of the next character that is likely to follow.\n",
        "\n",
        "And in addition to that, we are going to be able to evaluate any setting of the parameters of the langauage model, because we have a loss function value (The NLL).\n",
        "\n",
        "So we are going to look at the probability distributions and we are going to look at its labels (in the NN) which are basically the identity of the next character in the Bigram.\n",
        "\n",
        "So knowing what character comes next is the bigram, allows us to check what will be the probability value assigned to that character (So higher the value, the better. Because it is another way of saying that the loss is low).\n",
        "\n",
        "**We're gonna use gradient based optimization to tune the parameters of this network.**\n",
        "Because we have a loss function and we're gonna minimize it.\n",
        "We're gonna tune the weights, so that the NN is gonna correctly predict the next probability of the next characters."
      ],
      "metadata": {
        "id": "7T62sub4gIe6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gj3FsKQA7_d"
      },
      "outputs": [],
      "source": [
        "#Creating the training set of bigrams (x,y)\n",
        "xs, ys = [], []\n",
        "\n",
        "for word in words[:1]:\n",
        "    chs = ['.'] + list(word) + ['.']\n",
        "    for ch1, ch2 in zip(chs, chs[1:]):\n",
        "        ix1 = stoi[ch1]\n",
        "        ix2 = stoi[ch2]\n",
        "        print(ch1, ch2)\n",
        "        xs.append(ix1)\n",
        "        ys.append(ix2)\n",
        "\n",
        "xs = torch.tensor(xs)\n",
        "ys = torch.tensor(ys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRQtlzE3A7_d"
      },
      "outputs": [],
      "source": [
        "xs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvb40C7vA7_d"
      },
      "outputs": [],
      "source": [
        "ys"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feeding these examples into a neural network"
      ],
      "metadata": {
        "id": "PeXHiXgvgcST"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7oo50otvA7_d"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "xenc = F.one_hot(xs, num_classes=27).float() #IMP: manual type casting\n",
        "xenc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcZ9geSJA7_e"
      },
      "outputs": [],
      "source": [
        "xenc.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdEV6XMZA7_e"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJSCXFxXA7_e"
      },
      "outputs": [],
      "source": [
        "plt.imshow(xenc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1-vxbomA7_e"
      },
      "outputs": [],
      "source": [
        "W = torch.randn((27, 27))   #Generating the weights\n",
        "xenc @ W    #Doing matrix multiplication"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IQQY5I-A7_f"
      },
      "outputs": [],
      "source": [
        "logits = xenc @ W   #log-counts\n",
        "counts = logits.exp()   #equivalent to N matrix as before\n",
        "probs = counts / counts.sum(1, keepdims=True)   #Normalising the rows (To calculate the probability)\n",
        "probs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAYejROpA7_f"
      },
      "source": [
        "-------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jiOXDZa0A7_f"
      },
      "outputs": [],
      "source": [
        "xs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojcx31j6A7_f"
      },
      "outputs": [],
      "source": [
        "ys"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Randomly initialize 27 neurons' weights. each neuron receives 27 inputs"
      ],
      "metadata": {
        "id": "-wmYDkb7hF8r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F91D2cpoA7_g"
      },
      "outputs": [],
      "source": [
        "g = torch.Generator().manual_seed(210102123)\n",
        "W = torch.randn((27, 27), generator=g)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Input to the network: one-hot encoding"
      ],
      "metadata": {
        "id": "U1pYJYRkhJso"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3XyrmJGA7_g"
      },
      "outputs": [],
      "source": [
        "xenc = F.one_hot(xs, num_classes=27).float()\n",
        "logits = xenc @ W # predict log-counts\n",
        "counts = logits.exp() # counts, equivalent to N\n",
        "probs = counts / counts.sum(1, keepdims=True) # probabilities for next character"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the last 2 lines here are together called a 'softmax'"
      ],
      "metadata": {
        "id": "on4-1A1ChU7n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_g8agp0EA7_g"
      },
      "outputs": [],
      "source": [
        "probs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62gUON4KA7_g"
      },
      "outputs": [],
      "source": [
        "nlls = torch.zeros(5)\n",
        "for i in range(5):\n",
        "  # i-th bigram:\n",
        "  x = xs[i].item() # input character index\n",
        "  y = ys[i].item() # label character index\n",
        "  print('--------')\n",
        "  print(f'bigram example {i+1}: {itos[x]}{itos[y]} (indexes {x},{y})')\n",
        "  print('input to the neural net:', x)\n",
        "  print('output probabilities from the neural net:', probs[i])\n",
        "  print('label (actual next character):', y)\n",
        "  p = probs[i, y]\n",
        "  print('probability assigned by the net to the the correct character:', p.item())\n",
        "  logp = torch.log(p)\n",
        "  print('log likelihood:', logp.item())\n",
        "  nll = -logp\n",
        "  print('negative log likelihood:', nll.item())\n",
        "  nlls[i] = nll\n",
        "\n",
        "print('=========')\n",
        "print('average negative log likelihood, i.e. loss =', nlls.mean().item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8c6eMDuA7_h"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OPTIMIZATION**"
      ],
      "metadata": {
        "id": "B412Ax6yhg3n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snTHNHVJA7_h"
      },
      "outputs": [],
      "source": [
        "xs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVQfZKbgA7_i"
      },
      "outputs": [],
      "source": [
        "ys"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding the third parameter here (requires_grad=True) for the Backward pass"
      ],
      "metadata": {
        "id": "878F7UKnhplL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZAG-XvwA7_i"
      },
      "outputs": [],
      "source": [
        "g = torch.Generator().manual_seed(210102123)\n",
        "W = torch.randn((27, 27), generator=g, requires_grad=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FORWARD PASS"
      ],
      "metadata": {
        "id": "czl-Tfd4h2qc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvXNGH57A7_i"
      },
      "outputs": [],
      "source": [
        "xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
        "logits = xenc @ W # predict log-counts\n",
        "counts = logits.exp() # counts, equivalent to N\n",
        "probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
        "loss = -probs[torch.arange(5), ys].log().mean() #torch.arange(5) is basically 0 to 5(4) position, ys is from that tuple list | We calculate the probability values of that | Then we take their log values | Then we take their mean | Finally take the negative value (since NLL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axtQotosA7_i"
      },
      "outputs": [],
      "source": [
        "loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BACKWARD PASS"
      ],
      "metadata": {
        "id": "hYJri8T2h-yr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXBgHFf0A7_i"
      },
      "outputs": [],
      "source": [
        "W.grad = None\n",
        "loss.backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mo4P_RgcA7_i"
      },
      "outputs": [],
      "source": [
        "W.grad.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LOfiInNA7_j"
      },
      "outputs": [],
      "source": [
        "W.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "UPDATE\n"
      ],
      "metadata": {
        "id": "bzbmgrqbiERd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5o42FxiA7_j"
      },
      "outputs": [],
      "source": [
        "W.data += -0.1 * W.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYslD5p-A7_j"
      },
      "source": [
        "JUST PUTTING THEM TOGETHER TO PERFORM GRADIENT DESCENT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZI4Vw_mXA7_j"
      },
      "outputs": [],
      "source": [
        "#ONLY RUN THIS THE FIRST TIME\n",
        "# randomly initialize 27 neurons' weights. each neuron receives 27 inputs\n",
        "g = torch.Generator().manual_seed(210102123)\n",
        "W = torch.randn((27, 27), generator=g, requires_grad=True) #Adding the third parameter here for the Backward pass (as remember in micrograd we had done the same thing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFZ11_6ZA7_j"
      },
      "outputs": [],
      "source": [
        "#FORWARD PASS\n",
        "xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
        "logits = xenc @ W # predict log-counts\n",
        "counts = logits.exp() # counts, equivalent to N\n",
        "probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
        "loss = -probs[torch.arange(5), ys].log().mean() #torch.arange(5) is basically 0 to 5(4) position, ys is from that tuple list | We calculate the probability values of that | Then we take their log values | Then we take their mean | Finally take the negative value (since NLL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eq-mgYAbA7_k"
      },
      "outputs": [],
      "source": [
        "print(loss.item()) #CHECKING THE LOSS VALUE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgiI8YDfA7_k"
      },
      "outputs": [],
      "source": [
        "#BACKWARD PASS\n",
        "W.grad = None #the gradient is first set to zero\n",
        "loss.backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PE-9Y_DQA7_k"
      },
      "outputs": [],
      "source": [
        "#UPDATE\n",
        "W.data += -0.1 * W.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaz4AX9uA7_k"
      },
      "source": [
        "**PUTTING THEM ALL TOGETHER**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9uCxvdPA7_k"
      },
      "outputs": [],
      "source": [
        "# create the dataset\n",
        "xs, ys = [], []\n",
        "for w in words:\n",
        "  chs = ['.'] + list(w) + ['.']\n",
        "  for ch1, ch2 in zip(chs, chs[1:]):\n",
        "    ix1 = stoi[ch1]\n",
        "    ix2 = stoi[ch2]\n",
        "    xs.append(ix1)\n",
        "    ys.append(ix2)\n",
        "xs = torch.tensor(xs)\n",
        "ys = torch.tensor(ys)\n",
        "num = xs.nelement()\n",
        "print('number of examples: ', num)\n",
        "\n",
        "# initialize the 'network'\n",
        "g = torch.Generator().manual_seed(210102123)\n",
        "W = torch.randn((27, 27), generator=g, requires_grad=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tOhyPrXsA7_l"
      },
      "outputs": [],
      "source": [
        "# gradient descent\n",
        "for k in range(20):\n",
        "\n",
        "  # forward pass\n",
        "  xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
        "  logits = xenc @ W # predict log-counts\n",
        "  counts = logits.exp() # counts, equivalent to N\n",
        "  probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
        "  loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean()\n",
        "  print(loss.item())\n",
        "\n",
        "  # backward pass\n",
        "  W.grad = None # set to zero the gradient\n",
        "  loss.backward()\n",
        "\n",
        "  # update\n",
        "  W.data += -50 * W.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TV86FKDA7_l"
      },
      "source": [
        "SO WE ALMOST ACHIEVED A VERY LOW LOSS VALUE. SIMILAR TO THE LOSS VALUE WE CALCULATED WITHOUT NN, WHEN WE TYPED OUR OWN NAME AND SAW HOW IT PERFORMED."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYqpiEfqA7_l"
      },
      "source": [
        "Finally *drumrolls*, we are going to see how sampling from this model produces the outputs (Spoiler alert: it will be the same as how we made the model manually, coz... it is the same model just that we made it using Neural nets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKU47vEXA7_l"
      },
      "outputs": [],
      "source": [
        "# finally, sample from the 'neural net' model\n",
        "g = torch.Generator().manual_seed(210102123)\n",
        "\n",
        "for i in range(5):\n",
        "\n",
        "  out = []\n",
        "  ix = 0\n",
        "  while True:\n",
        "\n",
        "    # ----------\n",
        "    # BEFORE:\n",
        "    #p = P[ix]\n",
        "    # ----------\n",
        "    # NOW:\n",
        "    xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n",
        "    logits = xenc @ W # predict log-counts\n",
        "    counts = logits.exp() # counts, equivalent to N\n",
        "    p = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
        "    # ----------\n",
        "\n",
        "    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
        "    out.append(itos[ix])\n",
        "    if ix == 0:\n",
        "      break\n",
        "  print(''.join(out))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}